<!DOCTYPE html>
<html lang="en">

<head>
    <title>Some things I read in the first half of 2023 &#x2F;&#x2F; lynndotpy</title>
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    <meta name="robots" content="noodp"/>

    <link rel="stylesheet" href="https://lynndotpy.xyz/style.css">
    <link rel="stylesheet" href="https://lynndotpy.xyz/color/lynn-auto.css">

        <link rel="stylesheet" href="https://lynndotpy.xyz/color/background_auto.css">
    
    <link rel="stylesheet" href="https://lynndotpy.xyz/font-hack-subset.css">

        <link rel="alternate" type="application/rss+xml" title="RSS" href="https://lynndotpy.xyz/rss.xml">
    
        <link rel="shortcut icon" type="image/png" href="/favicon_package/favicon-32x32.png">
    
        
<link rel="stylesheet" href="https://lynndotpy.xyz/katex/katex.min.css">
<script defer src="https://lynndotpy.xyz/katex/katex.min.js"></script>
<script defer src="https://lynndotpy.xyz/katex/mathtex-script-type.min.js"></script>
<script defer src="https://lynndotpy.xyz/katex/auto-render.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        delimiters: [
          {left: "$$", right: "$$", display: true},
          {left: "$", right: "$", display: false},
          {left: "\\begin{equation}", right: "\\end{equation}", display: true},
          {left: "\\begin{align}", right: "\\end{align}", display: true},
          {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
          {left: "\\begin{gather}", right: "\\end{gather}", display: true},
          {left: "\\(", right: "\\)", display: false},
          {left: "\\[", right: "\\]", display: true}
      ]
      });
  });
</script>

    
    </head>

<body class="">
<div class="container">
    
    <header class="header">
        <div class="header__inner">
            <div class="header__logo">
                    
                <a href="https://lynndotpy.xyz/" style="text-decoration: none;">
                    <div class="logo">
                      
                            lynndotpy.xyz
                        
                    </div>
                </a>
            </div>
        </div>

        
        <nav class="menu">
            <ul class="menu__inner">
                <li><a href="https://lynndotpy.xyz/archive">archive</a></li>
            
                <li><a href="https://lynndotpy.xyz/about">about</a></li>
            
                <li><a href="https://lynndotpy.xyz/projects">projects</a></li>
            
                <li><a href="https://github.com/lynnpepin" target="_blank" rel="noopener noreferrer">github</a></li>
            
                <li><a href="https://gitlab.com/lynnpepin" target="_blank" rel="noopener noreferrer">gitlab</a></li>
            
                <li><a href="https://mastodon.social/@lynndotpy" target="_blank" rel="noopener noreferrer">mastodon</a></li>
            </ul>
        </nav>
    
    
        
    </header>
    

    <div class="content">
        
    <div class="post">
        
    <h1 class="post-title"><a href="https://lynndotpy.xyz/posts/reading-2023/">Some things I read in the first half of 2023</a></h1>
    <div class="post-meta-inline">
        
    <span class="post-date">
            2023-08-26
        </span>

    </div>

    
        <span class="post-tags-inline">
                :: tags:&nbsp;
                <a class="post-tag" href="https://lynndotpy.xyz/tags/post/">#post</a></span>
    

        
        <div class="post-content">
            <p>I read and Consumed™️ a number of things the first half of 2023. Let me detail some of them:</p>
<ul>
<li><em>Braiding Sweetgrass</em> by Robin Wall Kimmerer helped me reconcile disillusionment and frustration I had with academic science, and to help me appreciate different philosophies of thought.</li>
<li><em>The Magicians</em> by Lev Grossman is a grounded examination of the fantasy genre within a well-fleshed fantasy world. If &quot;Houlden Caulfield seeks Narnia but goes to Hogwarts&quot; sounds interesting, then check this out.</li>
<li><em>This is How You Lose the Time War</em> by Amal El-Mohtar and Max Gladstone is a short story about star-crossed lovers from two advanced, warring, timeline-hopping scifi empires. I didn't like it that much, but it was 200 pages and fun to read.</li>
<li>The <em>Before Me</em> podcast series is a well-constructed narrative built around interviews with the narrators mother, who arrived to America fleeing the Khmer Rouge in Cambodia. It's worth a listen.</li>
</ul>
<span id="continue-reading"></span><h1 id="how-braiding-sweetgrass-helped-me-come-to-terms-with-limitations-of-science-as-a-philosophy-and-practice">How &quot;Braiding Sweetgrass&quot; helped me come to terms with limitations of science as a philosophy and practice</h1>
<blockquote>
<p><strong>TLDR:</strong> Science^TM is good, but spending time as a scientist showed me the cracks.  disillusionment is hard to reconcile. If you can relate, I highly recommend reading Braiding Sweetgrass.</p>
</blockquote>
<p>During my years as a PhD student, I found myself increasingly frustrated with academia. I felt that researchers career incentives were not aligned with the pursuit of knowledge, and at least within machine learning, I felt that trends and assumptions go unchallenged (to the detriment of the field.)</p>
<p>I think of science as the best-effort cultural practice of knowledgemaking. I'd say science refers to, all at once,</p>
<ul>
<li>The act of <strong>research</strong>, or observing the world to learn new knowledge,</li>
<li>A <strong>protocol</strong> for articulating that knowledge to others,</li>
<li>A <strong>cultural practice</strong> for maintaining that knowledge between generations,</li>
<li>and a catch-all term for the <strong>knowledge</strong> that was gained through science.</li>
</ul>
<p>This is why doing research, writing journal articles, public outreach, and teaching classes are all jobs of A Scientist.</p>
<p>I understood that the Academic Science to which I belonged follows a single (flawed) tradition of knowledge (&quot;the scientific method&quot;) which can be traced back only a few thousand years. But I also knew that humanity worldwide had developed other ways of gaining and communicating knowledge, in the form of songs and stories, and much later, writing. That is to say, Academic Science was not the only science.</p>
<p>The only other major lineages of Science that I knew of (and took seriously) were Indigenous practices. I vaguely knew that a huge amount of our medicinal and agricultural knowledge could be traced to Indigenous people. More recently, Indigenous knowledge was able to supercede common scientific understanding, such as the practice of controlled burns. </p>
<p>I expressed this all to a friend (who was also a PhD student in machine learning) and she recommended <a href="https://en.wikipedia.org/wiki/Braiding_Sweetgrass">Robin Wall Kimmerer's 2013 &quot;Braiding Sweetgrass: Indigenous Wisdom, Scientific Knowledge, and the Teachings of Plants&quot;</a>. Part autobiography, part philosophy, part narrative, this book explores academic and indigenous sciences, humanities, and politics.</p>
<h2 id="an-aside-on-science-and-learning">An aside-- On science and learning</h2>
<p>I was a machine learning researcher. I spent a lot of time thinking about <em>how</em> we write algorithms which identify correlations and build useful models. We would call this &quot;learning&quot;.</p>
<p>The job of figuring out how to usefully model the world <em>is also the job of science!</em> That is to say, machine learning can be thought of <em>as a scientific method.</em> In this sense, the field of machine learning is a scientific field <em>about</em> a scientific method. </p>
<p>There are some good meta-papers analyzing the field of machine learning through the selfsame lens of machine learning. One interesting problem is <a href="https://proceedings.neurips.cc/paper/2019/file/ee39e503b6bedf0c98c388b7e8589aca-Paper.pdf">that of overfitting due to test-set re-use (see &quot;A Meta-Analysis of Overfitting in Machine Learning&quot; by Roelofs, Fridovich-Keil et. al, 2019).</a> Thinking of science <em>as</em> machine learning lets us frame our field as <em>overfitting</em>, and so lets us better identify and articulate problems in our field.</p>
<p>More generally, I think machine learning should be thought of as a <em>humanities</em>. After all, there's <em>no way</em> to be a machine learning researcher without constantly taking deep epistemological shrugs. No free lunch? Solomonoff complexity? ¯\_(ツ)_/¯ </p>
<h1 id="other-books-short-books-and-podcasts">Other books, short books, and podcasts</h1>
<p>I listened to an audiobook of <a href="https://en.wikipedia.org/wiki/The_Magicians_(Grossman_novel)"><strong>The Magicians</strong> by Lev Grossman</a> at the recommendation of a friend. This book series is <em>excellent</em>. Often described as &quot;Holden Caulfield goes to Hogwarts&quot;, it's a grounded and critical take on the fantasy genre. It subverts without being overly cynical and it grounds the story without taking the fun out. It's almost like science-fiction in some parts, where the author uncompromisingly works to make things <em>consistent</em>.</p>
<p>As much as <em>The Magicians</em> is critical of fantasy, it is a fantastic fantasy series on its own. The author is a grand storyteller, especially at setting scenes. Paragraphs are just descriptive enough to make the world feel tactile and real. Just be prepared to feel disdain for the main character. I ended up listening to the other two audiobooks in the series. I can wholeheartedly recommend the first one at the least.</p>
<p>~~I was also gifted a short book, <a href="https://en.wikipedia.org/wiki/The_Little_Book_of_Satanism">&quot;The Little Book of Satanism&quot; by La Carmina</a>, by a friend. It details the history of Satan as a mythological and literary figure, recent &quot;Satanist&quot; movements, as well as the recent history of &quot;The Satanic Temple&quot;. </p>
<p>~~I did appreciate learning more about &quot;The Satanic Temple&quot;, and how it is a distinct, different religion than the LaVeyan Church of Satan. In short, the TST is more lib-left than the CoS's lib-right, but both movements were born from contrarianness. I'm a big believer that <em>to be cringe is to be free,</em> so I can appreciate Satanist's inherent disregard for any worry about the label &quot;cringe&quot;.</p>
<p>~~I am writing this in 2023, a time where the American fascist movement is also entirely Christonationalist, so I see a lot of value in TST's activism and existence. The ironic thing is that TST's tenets line up pretty identically with the virtues Jesus Christ supposedly espoused. (And, given Jesus <em>did</em> fulfill the role and act of &quot;satan&quot; in the original Hebrew sense, and given Jesus <em>was</em> killed for opposing the state and existing religion, perhaps we could say Jesus was one of the first Satanists?)</p>
<p>I've also read the collection of short stories <a href="https://en.wikipedia.org/wiki/Squirrel_Seeks_Chipmunk">&quot;Squirrel Seeking Chipmunk&quot; by David Sedaris</a>. I appreciate how numerous and short they are, and they're generally high quality, but none were memorable enough to make this a &quot;must read&quot;. It makes me appreciate how accessible a printing press is, because I imagine this would have been a best-seller if it were written in the 1400s.</p>
<p>I also enjoyed  <a href="https://en.wikipedia.org/wiki/This_Is_How_You_Lose_the_Time_War">&quot;This Is How You Lose the Time War&quot; by Amal El-Mohtar and Max Gladstone</a>. As a book, I found it hard to follow and hard to appreciate, but enjoyable to read in short bursts. I'm usually a big fan of narratives which get creative with time, such as Homestuck or Tenet. I think it's difficult to craft a good narrative which proposes an interesting time mechanism (say, with two characters experiencing time in different directions). When crafted well, the literary ironies we learned in school can lap together like waves rolling over waves, as characters learn (or unlearn) key facts. The audience's view of the world is the one true timeline.</p>
<p>The science fiction in TIHYLTTW was simply too obtuse for me. The two main characters, Red and Blue, are soldiers in two different opposing factions, each with their own high-fantasy scifi world, each alone too large to fit into the occasional hint and glimpse. The end result are too many scattered pieces of information to allow me to piece together a coherent picture.</p>
<p>I also listened to several podcasts. I recommend <a href="https://www.beforemepodcast.com/episodes">the &quot;Before Me&quot; podcast series</a>. The narrator interviews her mother, a Cambodian-American refugee who arrived in the United States fleeing the Khmer Rouge in Cambodia. It's hard not to be constantly aware that the living memory of historical events are dwindling, dying, and disappearing with each passing year. This podcast is an effort to preserve some of that memory.</p>
<h1 id="other-good-papers-i-read-or-skimmed">Other good papers I read or skimmed</h1>
<p>When I read papers as a PhD, I was actively interested in considering how I would improve or involve this paper. Without the motivation of imminent implementation, I've found myself absorbing little from most papers.</p>
<p>These were ones that I read with enough interest to take something away.</p>
<ul>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922">&quot;On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜&quot; by Bender, Gebru et al (2021)</a> is a paper worth reading, but not worth surmising here.</li>
<li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf">&quot;Training language models to follow instructions with human feedback&quot; by OpenAI, 2022</a> is a primer on RLHF.
<ul>
<li>Aside: My very first completed learning project was a senior design NLP project where my team and I converted BDD descriptions to executable test-cases. In a last-ditch effort to fine tune the model the night before demo day, I made a simple REPL that let us train the model by hand, epoch-by-epoch. We took shifts training the data. It made the demo a bit better!</li>
</ul>
</li>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3530811">&quot;Efficient Transformers: A Survey&quot; by Yi Tay et al, 2022</a>-- this is a paper I skimmed when I was interested in implementing a transformer model for a Homestuck bot. I ended up not creating the bot.</li>
<li>I also read a bit on diffusion models, as I had not read about them <em>at all</em> during my time as a PhD student, so I took a quick review.
<ul>
<li><a href="https://arxiv.org/pdf/2209.04747.pdf">&quot;Diffusion Models in Vision: A Survey&quot; by Croitoru, Hondru, Ionescu, Shah 2023</a>. I skimmed this when I wanted to create the aforementioned Homestuck bot, but then relented and decided to read the original:</li>
<li><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf">&quot;High-Resolution Image Synthesis with Latent Diffusion Models&quot; by Rombach, Blattmann, et. al 2022</a>. This was another one that I only skimmed, coming to realize that I didn't care about that Homestuck shitpost bot so much.</li>
</ul>
</li>
</ul>

        </div>

        
        <div class="pagination">
            <div class="pagination__title">
                <span class="pagination__title-h"></span>
                <hr />
            </div>
            <div class="pagination__buttons">
                    <span class="button previous">
                        <a href="https://lynndotpy.xyz/posts/caddy-topics-optout/">
                            <span class="button__icon">←</span>&nbsp;
                            <span class="button__text">How to Opt out of Chrome Topics API on Caddy servers</span>
                        </a>
                    </span>
                
                
                    <span class="button next">
                        <a href="https://lynndotpy.xyz/posts/recommendations/">
                            <span class="button__text">Software I recommend</span>&nbsp;
                            <span class="button__icon">→</span>
                        </a>
                    </span>
                </div>
        </div>
    
    </div>

    </div>


    
    <footer class="footer">
        <div class="footer__inner">
                <div class="copyright copyright--user">© Lynn Pepin, and <a href='https://creativecommons.org/licenses/by-nc-sa/4.0/'>cc by-nc-sa</a> except where specified.<br>Based on the <a href='https://github.com/pawroman/zola-theme-terminimal/'>Terminal theme</a> by  pawroman, panr</div>
            </div>
    </footer>
    

</div>
</body>

</html>
